# -*- coding: utf-8 -*-
"""MINST_handwrite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WYPXwW_40ie8Rgf44qB3XewhHTGdL8i5
"""

from keras.models import Model
from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization
from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
from PIL import Image, ImageEnhance
import numpy as np
import os
import csv
import cv2
import time
import random
from matplotlib import pyplot as plt

"""### Model"""

# Create CNN Model
print("Creating CNN model...")
input_data = Input((28, 28, 1))
out = input_data
out = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(out)
out = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(out)
out = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(out)
out = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(out)
out = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(out)
out = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(out)
out = BatchNormalization()(out)
out = MaxPooling2D(pool_size=(2, 2))(out)
out = Dropout(0.2)(out)
out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(out)
out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(out)
out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(out)
out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(out)
out = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(out)
out = BatchNormalization()(out)
out = MaxPooling2D(pool_size=(2, 2))(out)
out = Flatten()(out)
out = Dropout(0.25)(out)
out = [Dense(10, name='digit', activation='softmax')(out)]
model = Model(inputs=input_data, outputs=out)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""### Prepareing Data"""

LETTERSTR = '0123456789'
def toonehot(text):
    labellist = []
    for letter in text:
        onehot = [0 for _ in range(10)]
        num = LETTERSTR.find(letter)
        onehot[num] = 1
        labellist.append(onehot)
    return labellist[0]

train_root = '/content/drive/MyDrive/Algorithm/Midterm/train_image'
train_path = []
for file in os.listdir(train_root):
  tmp_path = os.path.join(train_root, file)
  for d in os.listdir(tmp_path):
    train_path.append(os.path.join(tmp_path, d))
random.seed(20)
random.shuffle(train_path)
train_path[0:5]

data_path = []
train_label = []
for i in range(len(train_path)):
  for root, dirs, files in os.walk(train_path[i]):
    for n in range(len(files)):
      data_path.append(os.path.join(root, files[n]))
      train_label.append(toonehot(root[-1]))
print('checkpoint: ', train_label[0:5])
print('data length: ', len(data_path))
print('The shape of labels: ', np.shape(train_label))

train_label = np.array(train_label)

"""### Preprocessing"""

def cv_imread(filePath):
    cv_img=cv2.imdecode(np.fromfile(filePath,dtype=np.uint8), cv2.IMREAD_COLOR)
    img_gray = cv2.cvtColor(cv_img, cv2.COLOR_RGB2GRAY)
    return img_gray

#img = [cv_imread(data_path[i]) for i in range(len(data_path))]
#np.save('train_img', img)

train_matrix = np.load('/content/drive/MyDrive/Algorithm/Midterm/train_img.npy')
train_matrix.shape

train_data = np.stack([np.array(i)/255.0 for i in train_matrix])
print("The dimension of training data: ", train_data.shape)

"""### Modeling Training"""

#my_stopping = EarlyStopping(patience=3, monitor = 'val_accuracy')
my_tensorboard = TensorBoard(log_dir = ".\\log", histogram_freq = 1)
my_callback = my_tensorboard #[my_stopping, my_tensorboard]
training_history = model.fit(train_data, train_label, epochs=30, verbose = 1 , batch_size=32, validation_split=0.2, callbacks=my_callback)

x = np.arange(0, 30, 1)
y1 = training_history.history['val_loss']
y2 = np.array(training_history.history['val_accuracy']) * 100
fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.plot(x, y1)
ax1.set_ylabel("val_loss")
ax1.set_xlabel("Epoch")

ax2 = ax1.twinx()
ax2.plot(x, y2, "r")
ax2.set_ylabel("val_acc")

plt.title("Train History")
plt.xlabel("Epoch")
fig.legend(["val_loss", "val_acc"], loc = "upper left")
plt.show()

model.save('./Algo_midterm'+str(int(time.time()))+'.h5')





